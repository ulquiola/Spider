- 场景一
防：检测请求头中的字段，比如：User-Agent、referer等字段。
攻：只要在 http 请求的 headers 中带上对应的字段即可。

- 场景二
防：后台对访问的 IP 进行统计，如果单个 IP 访问超过设定的阈值，给予封锁。虽然这种方法效果还不错， 但是其实有两个缺陷。 一个是非常容易误伤普通用户， 另一个就是 IP 其实不值钱， 各种代理网站都有出售大量的 IP 代理地址。 所以建议加大频率周期,每小时或每天超过一定次数屏蔽 IP 一段时间（不提示时间）。
攻：针对这种情况，可通过使用代理服务器解决。同时，爬虫设置下载延迟，每隔几次请求，切换一下所用代理的IP地址。

- 场景三
防：后台对访问进行统计， 如果单个 userAgent 访问超过阈值， 予以封锁。这种方法拦截爬虫效果非常明显，但是杀伤力过大，误伤普通用户概率非常高。所以要慎重使用。
攻：收集大量浏览器的 userAgent 即可。

- 场景四
防：网站对访问有频率限制，还设置验证码。增加验证码是一个既古老又相当有效果的方法。能够让很多爬虫望风而逃。而且现在的验证码的干扰线, 噪点都比较多，甚至还出现了人类肉眼都难以辨别的验证码（12306 购票网站）。
攻：python+tesseract 验证码识别库模拟训练，或使用类似 tor 匿名中间件（广度遍历IP）

- 场景五
防：网站页面是动态页面，采用 Ajax 异步加载数据方式来呈现数据。这种方法其实能够对爬虫造成了绝大的麻烦。
攻：首先用 Firebug 或者 HttpFox 对网络请求进行分析。如果能够找到 ajax 请求，也能分析出具体的参数和响应的具体含义。则直接模拟相应的http请求，即可从响应中得到对应的数据。这种情况，跟普通的请求没有什么区别。
- 能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把 ajax 请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求，请看场景六。

- 场景六
防：基于 JavaScript 的反爬虫手段，主要是在响应数据页面之前，先返回一段带有JavaScript 代码的页面，用于验证访问者有无 JavaScript 的执行环境，以确定使用的是不是浏览器。例如淘宝、快代理这样的网站。
- 这种反爬虫方法。通常情况下，这段JS代码执行后，会发送一个带参数key的请求，后台通过判断key的值来决定是响应真实的页面，还是响应伪造或错误的页面。因为key参数是动态生成的，每次都不一样，难以分析出其生成方法，使得无法构造对应的http请求。
攻：采用 selenium+phantomJS 框架的方式进行爬取。调用浏览器内核，并利用phantomJS 执行 js 来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。
